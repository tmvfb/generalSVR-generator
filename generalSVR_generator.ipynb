{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tmvfb/generalSVR-generator/blob/main/generalSVR_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> (old notebook version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BO7MEGbb6mtB"
   },
   "source": [
    "# Finetune RuGPT model with certain telegram channel content  \n",
    "RuGPT3Small model taken from [here](https://huggingface.co/ai-forever/rugpt3small_based_on_gpt2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Njd0y5VT6Gfj"
   },
   "source": [
    "## Install env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "csHcDJXFDdaW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJxPg-cJDhAB",
    "outputId": "be70959a-8d55-49bc-c58a-37d72f5b133a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0d93b31210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJZtWu8u6nwL",
    "outputId": "a6465117-adea-4c30-8548-f458b1e510a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models/’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqwZiumW8WbZ"
   },
   "source": [
    "## Create files and build train/validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 parser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "OXdNbrq3rgzq",
    "outputId": "e87e032e-84ab-4b62-ff3b-abe02af73857"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Дорогие подписчики и гости канала! Канал \"Гене...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Добрый вечер, дорогие подписчики и гости канал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Здравствуйте, дорогие наши подписчики и гости ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Здравствуйте, дорогие подписчики и гости канал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Здравствуйте, дорогие подписчики и гости канал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Здравствуйте, дорогие подписчики и гости канал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Здравствуйте, дорогие наши подписчики и гости ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Здравствуйте, дорогие наши подписчики и гости ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Дорогие подписчики и гости канала! Завтра, во ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Дорогие подписчики и гости канала! Сегодня мат...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Дорогие подписчики и гости канала! Канал \"Гене...\n",
       "1  Добрый вечер, дорогие подписчики и гости канал...\n",
       "2  Здравствуйте, дорогие наши подписчики и гости ...\n",
       "3  Здравствуйте, дорогие подписчики и гости канал...\n",
       "4  Здравствуйте, дорогие подписчики и гости канал...\n",
       "5  Здравствуйте, дорогие подписчики и гости канал...\n",
       "6  Здравствуйте, дорогие наши подписчики и гости ...\n",
       "7  Здравствуйте, дорогие наши подписчики и гости ...\n",
       "8  Дорогие подписчики и гости канала! Завтра, во ...\n",
       "9  Дорогие подписчики и гости канала! Сегодня мат..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/parsed_data.json\"\n",
    "data = pd.read_json(data_path, encoding=\"utf-8\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IWaigRvyxmT",
    "outputId": "9d643805-701e-4a9e-e6b7-3e3d9968af4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "j0nbtCxLzTfM"
   },
   "outputs": [],
   "source": [
    "val_ind = random.sample(range(data.shape[0]), 150)\n",
    "train = [data.iloc[i][0] for i in range(len(data)) if i not in val_ind]\n",
    "valid = [data.iloc[i][0] for i in range(len(data)) if i in val_ind]\n",
    "# train = list(np.random.choice(data.iloc[:, 0], size=1100))\n",
    "# valid = list(np.random.choice(data.iloc[:, 0], size=250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2T0gN6gqr9pa",
    "outputId": "85503e93-86cc-45b9-9714-951456f29e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(781, 150)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘artifacts’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZPB8rrVPr-kh"
   },
   "outputs": [],
   "source": [
    "with open(\"artifacts/train.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HP5_nk_0sAB0"
   },
   "outputs": [],
   "source": [
    "with open(\"artifacts/valid.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoyX62qN_38l"
   },
   "source": [
    "## Train \n",
    "The following code downloads RuGPT model and tokenizer from huggingface and finetunes model for generating essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SD6jS49-TAbM",
    "outputId": "d404dcfd-f2c6-4a93-a297-b778d1350244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-06 17:25:12--  https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_clm.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28779 (28K) [text/plain]\n",
      "Saving to: ‘run_clm.py.3’\n",
      "\n",
      "run_clm.py.3        100%[===================>]  28.10K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-12-06 17:25:13 (2.21 MB/s) - ‘run_clm.py.3’ saved [28779/28779]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_clm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCIERP8AS1Dl",
    "outputId": "c8919956-48d6-4863-d76c-ed4127b6e335",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 17:25:16.361740: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-06 17:25:16.609240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-06 17:25:17.810618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "12/06/2023 17:25:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "12/06/2023 17:25:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/essays/runs/Dec06_17-25-20_mv,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=models/essays,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/essays,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Using custom data configuration default-f967efb5cac4e96e\n",
      "12/06/2023 17:25:21 - INFO - datasets.builder - Using custom data configuration default-f967efb5cac4e96e\n",
      "Loading Dataset Infos from /home/tmvfb/.local/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "12/06/2023 17:25:21 - INFO - datasets.info - Loading Dataset Infos from /home/tmvfb/.local/lib/python3.10/site-packages/datasets/packaged_modules/text\n",
      "Generating dataset text (/home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)\n",
      "12/06/2023 17:25:21 - INFO - datasets.builder - Generating dataset text (/home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)\n",
      "Downloading and preparing dataset text/default to /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...\n",
      "12/06/2023 17:25:21 - INFO - datasets.builder - Downloading and preparing dataset text/default to /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 19021.79it/s]\n",
      "Downloading took 0.0 min\n",
      "12/06/2023 17:25:21 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "12/06/2023 17:25:21 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1334.70it/s]\n",
      "Generating train split\n",
      "12/06/2023 17:25:21 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 781 examples [00:00, 23498.26 examples/s]\n",
      "Generating validation split\n",
      "12/06/2023 17:25:21 - INFO - datasets.builder - Generating validation split\n",
      "Generating validation split: 150 examples [00:00, 44855.67 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "12/06/2023 17:25:21 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.\n",
      "12/06/2023 17:25:21 - INFO - datasets.builder - Dataset text downloaded and prepared to /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34. Subsequent calls will reuse this data.\n",
      "config.json: 100%|█████████████████████████████| 608/608 [00:00<00:00, 3.30MB/s]\n",
      "[INFO|configuration_utils.py:718] 2023-12-06 17:25:22,053 >> loading configuration file config.json from cache at /home/tmvfb/.cache/huggingface/hub/models--ai-forever--rugpt3small_based_on_gpt2/snapshots/22144487b85433850603395464c6ee8296649d47/config.json\n",
      "[INFO|configuration_utils.py:778] 2023-12-06 17:25:22,054 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"ai-forever/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.36.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:577] 2023-12-06 17:25:22,180 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:718] 2023-12-06 17:25:22,308 >> loading configuration file config.json from cache at /home/tmvfb/.cache/huggingface/hub/models--ai-forever--rugpt3small_based_on_gpt2/snapshots/22144487b85433850603395464c6ee8296649d47/config.json\n",
      "[INFO|configuration_utils.py:778] 2023-12-06 17:25:22,309 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"ai-forever/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.36.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "vocab.json: 100%|██████████████████████████| 1.71M/1.71M [00:00<00:00, 3.08MB/s]\n",
      "merges.txt: 100%|██████████████████████████| 1.27M/1.27M [00:00<00:00, 12.2MB/s]\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-12-06 17:25:24,240 >> loading file vocab.json from cache at /home/tmvfb/.cache/huggingface/hub/models--ai-forever--rugpt3small_based_on_gpt2/snapshots/22144487b85433850603395464c6ee8296649d47/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-12-06 17:25:24,240 >> loading file merges.txt from cache at /home/tmvfb/.cache/huggingface/hub/models--ai-forever--rugpt3small_based_on_gpt2/snapshots/22144487b85433850603395464c6ee8296649d47/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-12-06 17:25:24,240 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-12-06 17:25:24,240 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-12-06 17:25:24,240 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2026] 2023-12-06 17:25:24,240 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:718] 2023-12-06 17:25:24,241 >> loading configuration file config.json from cache at /home/tmvfb/.cache/huggingface/hub/models--ai-forever--rugpt3small_based_on_gpt2/snapshots/22144487b85433850603395464c6ee8296649d47/config.json\n",
      "[INFO|configuration_utils.py:778] 2023-12-06 17:25:24,241 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"ai-forever/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.36.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:718] 2023-12-06 17:25:24,373 >> loading configuration file config.json from cache at /home/tmvfb/.cache/huggingface/hub/models--ai-forever--rugpt3small_based_on_gpt2/snapshots/22144487b85433850603395464c6ee8296649d47/config.json\n",
      "[INFO|configuration_utils.py:778] 2023-12-06 17:25:24,374 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"ai-forever/rugpt3small_based_on_gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.36.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "pytorch_model.bin: 100%|█████████████████████| 551M/551M [00:48<00:00, 11.3MB/s]\n",
      "[INFO|modeling_utils.py:3196] 2023-12-06 17:26:13,883 >> loading weights file pytorch_model.bin from cache at /home/tmvfb/.cache/huggingface/hub/models--ai-forever--rugpt3small_based_on_gpt2/snapshots/22144487b85433850603395464c6ee8296649d47/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:818] 2023-12-06 17:26:14,500 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4034] 2023-12-06 17:26:16,161 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4042] 2023-12-06 17:26:16,161 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ai-forever/rugpt3small_based_on_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|modeling_utils.py:3600] 2023-12-06 17:26:16,292 >> Generation config file not found, using a generation config created from the model config.\n",
      "Running tokenizer on dataset:   0%|              | 0/781 [00:00<?, ? examples/s]Caching processed dataset at /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-8e7e769e6fe9a5a1.arrow\n",
      "12/06/2023 17:26:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-8e7e769e6fe9a5a1.arrow\n",
      "Running tokenizer on dataset: 100%|██| 781/781 [00:00<00:00, 3159.50 examples/s]\n",
      "Running tokenizer on dataset:   0%|              | 0/150 [00:00<?, ? examples/s]Caching processed dataset at /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-b8ad60d3e2c5552c.arrow\n",
      "12/06/2023 17:26:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-b8ad60d3e2c5552c.arrow\n",
      "Running tokenizer on dataset: 100%|██| 150/150 [00:00<00:00, 3804.20 examples/s]\n",
      "Grouping texts in chunks of 2048:   0%|          | 0/781 [00:00<?, ? examples/s]Caching processed dataset at /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-7e3980e69ba4f378.arrow\n",
      "12/06/2023 17:26:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-7e3980e69ba4f378.arrow\n",
      "Grouping texts in chunks of 2048: 100%|█| 781/781 [00:00<00:00, 4083.76 examples\n",
      "Grouping texts in chunks of 2048:   0%|          | 0/150 [00:00<?, ? examples/s]Caching processed dataset at /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-99d39c26a44b077f.arrow\n",
      "12/06/2023 17:26:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tmvfb/.cache/huggingface/datasets/text/default-f967efb5cac4e96e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-99d39c26a44b077f.arrow\n",
      "Grouping texts in chunks of 2048: 100%|█| 150/150 [00:00<00:00, 3793.92 examples\n",
      "[INFO|trainer.py:1699] 2023-12-06 17:26:25,696 >> ***** Running training *****\n",
      "[INFO|trainer.py:1700] 2023-12-06 17:26:25,696 >>   Num examples = 140\n",
      "[INFO|trainer.py:1701] 2023-12-06 17:26:25,696 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1702] 2023-12-06 17:26:25,696 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1705] 2023-12-06 17:26:25,696 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1706] 2023-12-06 17:26:25,696 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1707] 2023-12-06 17:26:25,696 >>   Total optimization steps = 420\n",
      "[INFO|trainer.py:1708] 2023-12-06 17:26:25,696 >>   Number of trainable parameters = 125,231,616\n",
      "100%|███████████████████████████████████████| 420/420 [6:49:51<00:00, 16.43s/it][INFO|trainer.py:1940] 2023-12-07 00:16:17,339 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 24592.1228, 'train_samples_per_second': 0.017, 'train_steps_per_second': 0.017, 'train_loss': 2.9580955868675596, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████| 420/420 [6:49:52<00:00, 58.55s/it]\n",
      "[INFO|trainer.py:2858] 2023-12-07 00:16:17,931 >> Saving model checkpoint to models/essays\n",
      "[INFO|configuration_utils.py:462] 2023-12-07 00:16:17,948 >> Configuration saved in models/essays/config.json\n",
      "[INFO|configuration_utils.py:586] 2023-12-07 00:16:17,957 >> Configuration saved in models/essays/generation_config.json\n",
      "[INFO|modeling_utils.py:2246] 2023-12-07 00:16:37,427 >> Model weights saved in models/essays/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2432] 2023-12-07 00:16:37,449 >> tokenizer config file saved in models/essays/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2441] 2023-12-07 00:16:37,450 >> Special tokens file saved in models/essays/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =     2.9581\n",
      "  train_runtime            = 6:49:52.12\n",
      "  train_samples            =        140\n",
      "  train_samples_per_second =      0.017\n",
      "  train_steps_per_second   =      0.017\n",
      "12/07/2023 00:16:37 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3135] 2023-12-07 00:16:37,671 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3137] 2023-12-07 00:16:37,671 >>   Num examples = 25\n",
      "[INFO|trainer.py:3140] 2023-12-07 00:16:37,671 >>   Batch size = 1\n",
      "100%|███████████████████████████████████████████| 25/25 [00:42<00:00,  1.71s/it]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_accuracy           =     0.4371\n",
      "  eval_loss               =     3.0366\n",
      "  eval_runtime            = 0:00:44.60\n",
      "  eval_samples            =         25\n",
      "  eval_samples_per_second =      0.561\n",
      "  eval_steps_per_second   =      0.561\n",
      "  perplexity              =    20.8335\n",
      "[INFO|modelcard.py:452] 2023-12-07 00:17:22,963 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.43714704445530045}]}\n"
     ]
    }
   ],
   "source": [
    "!python3 run_clm.py \\\n",
    "    --model_name_or_path ai-forever/rugpt3small_based_on_gpt2 \\\n",
    "    --train_file artifacts/train.txt \\\n",
    "    --validation_file artifacts/valid.txt \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --block_size 2048 \\\n",
    "    --dataset_config_name plain_text \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir models/essays \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvgntLymArg3"
   },
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x_EMbgO0BTvb"
   },
   "outputs": [],
   "source": [
    "tok = GPT2Tokenizer.from_pretrained(\"models/essays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Fjy0GAuQBYpA"
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"models/essays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irh4H-HDBb6V",
    "outputId": "aa36d271-918d-4aea-e3f6-3ab3e5f4eaa5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50264, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hQY6A5q7Bd4O"
   },
   "outputs": [],
   "source": [
    "text = \"Дорогие подписчики и гости канала! Владимир Путин съел яичницу на завтрак.\"\n",
    "inpt = tok.encode(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gfJFmeOBj_t",
    "outputId": "e9e123ac-94fb-4f49-c3ea-8ba15a800aa5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(\n",
    "    inpt.cuda(), max_length=100, repetition_penalty=5.0, do_sample=True, top_k=20, top_p=1, temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "gWZ9SUCxB2Ki",
    "outputId": "fc94c8a1-b0f3-47ef-b84e-22164c64c33e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Дорогие подписчики и гости канала! Владимир Путин съел яичницу на завтрак. Несмотря, что в утреннем меню были исключительно омлет с колбасой (на мой вкус-не особо изысканно), у стола президента во вторник была не просто двойная порция яиц разных видов плюс несколько десятков грамм сыра «Бургер Кинг»… А ещё президент провел совещание по вопросам безопасности полетов беспилотных летательных аппаратов над территорией РФ для руководства военного блока страны.. И если вчера всё выглядело более убедительно при подготовке к встрече'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(out[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
